function [final_parameters, epsilon, omega] = dp_spls_updated_AR_DP(analysis_folder, X, Y, hp, tp, B, W, K)
%
%   Implementation of SPLS algorithm and framework as described in Monteiro
%   et al. 2016, for details: doi:10.1016/j.jneumeth.2016.06.011
%   
% Inputs:
%
%     X, Y    - data matrices for MRI images (X) and behavioral data (Y) in
%               the form: samples x features. Use MRI data directly from
%               Neurominer output, the function will standardize both the
%               MRI data and the behavioral data, so that mean = 0 and std =
%               1.
% 
%     hp      - Ratio (in %) of hold out data, which is projected on both
%               the original data set with optimal cu/cv and on the
%               permuted dataset with optimal cu/cv. Default: 10 
% 
%     tp      - Ratio (in %) of the test data within the K-fold loops.
%               Default: 20 
%
%     B       - Number of Permutations for the statistical evaluation of
%               the discovered cu/cv combination. Default: 10000
% 
%     W       - Number of wide loops that define how many times the entire
%               SPLS procedure is repeated and how many final p values are
%               calculated. Default: 10
%
%     K       - Number of K splits for the training and testing of the held
%               in data for each cu/cv combination. Default: 100
%   
% 
% Outputs: 
% 
%     final_parameters  - Results matrix with rows = number of significant
%                       LVs and columns containing the following variables
%                       w, cu_opt, cv_opt, u_opt, v_opt, success_opt,
%                       RHO_opt, p. The last row contains the first LV that
%                       was not significant, it is for informative purposes
%                       only.
%
%     epsilon           - Matrix with rows = Number of LV and columns
%                       containing the projection of the weight vector u of
%                       the LV on X.
%
%     omega             - Matrix with rows = Number of LV and columns
%                       containing the projection of the weight vector v of
%                       the LV on Y. 
%
%
%   Version: 2018-03-05
%__________________________________________________________________________
%
% Written by David Popovic 
% Email: david.popovic@med.uni-muenchen.de
%

%% make a new directory for temporary files and folders
cd(folder)
mkdir('temp_RHO');
RHO_path = [folder '/temp_RHO'];
mkdir('temp_B');
B_path = [folder '/temp_B'];
mkdir('common');
common_path = [folder '/common'];

%% 1. Preparation of SPLS and assignment of matrices
% Please make sure that you have added the path for the folder containing
% the following function: spls.m, dp_spls.m, dp_partition_holdout.m,
% dp_perm.m
% standardize the data matrices X and Y for usage in SPLS. X_original and
% Y_original will later be used for projection onto the final SPLS latent
% space spanned by u and v

X_original = zscore(X);
Y_original = zscore(Y);

% define the data matrices X and Y for input into the SPLS algorithm
X = X_original;
Y = Y_original;

% define the default values for variables tp, hp, B, W, K if no value for
% them is entered

% hp ratio
if ~exist('hp', 'var')
    hp = 10;
end

% tp ratio
if ~exist('tp', 'var')
    tp = 20;
end

% B
if ~exist('B', 'var')
    B = 10000;
end

% W
if ~exist('W', 'var')
    W = 10;
end

% K
if ~exist('K', 'var')
    K = 100;
end

%% 2. LV loop

% this loop is repeated for the next vector pair u and v as long as the
% previous vector pair was significant. If the previous vector was not
% significant, then the loop will stop

% pLV is a starting p value set to 0, so that the while loop starts. Later
% on it will be updated at the end of each LV iteration. Then it serves the
% purpose to stop the while loop as soon as an LV was not significant.
p_LV = 0; 

% FWE_rate corrects for multiple testing for the W amount of p values
FWE_rate = 0.05 / W; 

% ff counts the iteration through the LV loops, so that the results for each
% LV can be stored in separate rows
ff = 1;  

% cu_range and cv_range define the range for the grid search. The grid
% search is performed along 40 points from 1 to sqrt(number of variables)
% as proposed in Monteiro et al. 2016.
range_points = 40; % defines the number of range points for the grid search. 40 is the proposed amount.
cu_range = linspace(1,sqrt(size(X,2)),range_points);
cv_range = linspace(1,sqrt(size(Y,2)),range_points);

% compile a matrix with separate row for all possible cu and cv
% combinations by taking cu and repeating every single element 40 times and
% then takin cv and repeating the entire vector 40 times
cu_cv_combination = [repelem(cu_range,numel(cu_range));repmat(cv_range,1,numel(cv_range))]; % this matrix contains all the possible combinations between cu and cv

% define column names for matrices so that you can access them later by
% indexing
spls_output_k_names = {'u','v','success', 'RHO'}; % output of each k iteration
final_parameters_names = {'w', 'cu', 'cv', 'u', 'v', 'success', 'RHO', 'p'};
opt_parameters_names = {'w', 'cu', 'cv', 'u', 'v', 'success', 'RHO', 'p'}; % names of parameters for optimal cu/cv combination of the w-th loop
cu_cv_combination_names = {'cu';'cv'};

% preallocate placeholder matrices for higher speed
final_parameters = num2cell(zeros(size(Y,2), numel(final_parameters_names))); % cell array to store final parameters for each LV iteration
epsilon = zeros(size(final_parameters,1),size(X_original,1)); %projection of weight vector u on corresponding matrix X
omega = zeros(size(final_parameters,1),size(Y_original,1)); % projection of weight vector v on corresponding matrix Y

% indices for later use
index_u = strcmp(opt_parameters_names,'u');
index_v = strcmp(opt_parameters_names,'v');
index_cu = strcmp(cu_cv_combination_names,'cu');
index_cv = strcmp(cu_cv_combination_names,'cv');
index_RHO_k = strcmp(spls_output_k_names,'RHO'); 
index_RHO = strcmp(opt_parameters_names, 'RHO');
index_p = strcmp(opt_parameters_names, 'p');  

% here starts the outer loop for each single LV, the next LV is only
% computed if the previous LV was significant (with FWE correction)
while p_LV <= FWE_rate
 
%% 3. Wide loop
% repeats the entire process W times to obtain W different final p values,
% which go into the omnibus hypothesis

% matrix to store the optimal parameters of the w-th loop
opt_parameters = num2cell(zeros(W,numel(opt_parameters_names)));

    for w=1:W

        %% hyper-parameter optimisation
        % remove hp% of the data randomly and keep it in a hold-out
        % dataset, the rest is called the keep_in data for training/testing
        % the algorithm

        [hold_out_data_x, keep_in_data_x, hold_out_data_y, keep_in_data_y] = dp_partition_holdout(hp, X, Y);

        % create a vector which is updated for every i-th hyperparameter
        % combination
        RHO_avg = zeros(size(cu_cv_combination,2),1);
        success_cu_cv = zeros(size(cu_cv_combination,2),1);
%         saveopt = '-v7.3';
%         save('/volume/DP_FEF/Analysis/28-Mar-2018/cvcu/RHO_avg_success.mat','RHO_avg','success_cu_cv', saveopt);
%          
        % RHO_avg is a vector where all average RHO values of the different
        % cu/cv combinations are stored, preallocated for speed
        save([common_path '/cucv.mat'],...
            'cu_cv_combination',...
            'keep_in_data_x',...
            'keep_in_data_y',...
            'tp',...
            'K',...
            'RHO_path');
%         save('/volume/DP_FEF/Analysis/28-Mar-2018/cvcu/RHO_avg.mat','RHO_avg');
        cd(analysis_folder);
        if ~exist([analysis_folder '/temp_RHO'])
            mkdir temp_RHO; 
        end
        
        if ~exist([analysis_folder '/temp_B'])
            mkdir temp_B;
        end
        
        cd([analysis_folder '/temp_RHO']);
        success = system('qsub /volume/DP_FEF/Analysis/12-April-2018/dp_rho_avg_testing.sh');

RHO_collection = zeros(10,1);

% for i=1:10
%     load(['RHO_avg_' num2str(i) '.mat'], 'RHO_avg');
%     RHO_collection(i,1) = RHO;
% end
%         
% collect all files with RHO values, open them and store the RHO values in
% a double
% folder = '/volume/DP_FEF/Analysis/28-Mar-2018/cvcu';
filecount = size(dir([analysis_folder '/temp_RHO/RHO_avg_*']),1);
RHO_collection = zeros(size(cu_cv_combination,2),1);


   
for i=1:size(cu_cv_combination,2)
    if exist(['RHO_avg_' num2str(i) '.mat'])
        load(['/volume/DP_FEF/Analysis/28-Mar-2018/cvcu/RHO_avg_',num2str(i),'.mat']);
        RHO_collection(i) = RHO_avg;
    else
        RHO_collection(i) = NaN;
    end
end


%     select the hyperparameter combination with the highest average
%     correlation, ie the one with the highest RHO_avg
        [value_RHO, index_RHO] = max(RHO_collection); 
        
%     find the corresponding cu and cv values of the max RHO element, this
%     gives you the cu/cv combination with the highest average correlation
    
    cu_opt = cu_cv_combination(index_cu,index_RHO);
    cv_opt = cu_cv_combination(index_cv,index_RHO);

%     for i=1:size(cu_cv_combination,2)
%         filename = fullfile(['/volume/DP_FEF/Analysis/28-Mar-2018/cvcu/RHO_avg_',num2str(i),'.mat']);
%         delete(filename);
%     end


    %% Statistical Evaluation

    
    %train the model with the optimal cu/cv combination on the previous
    %train/test data set to get u and v
    
    [u_opt, v_opt, success_opt]=spls(keep_in_data_x, keep_in_data_y,cu_opt, cv_opt);

    %project the hold_out data set on the computed u and v vectors to
    %get the absolute correlations between these projections
    RHO_opt = abs(corr(hold_out_data_x*u_opt,hold_out_data_y*v_opt));

    % permute the keep_in data in one dimension so that the
    % relationship between these two views gets destroyed
    
    % collection of RHO values for the B times permutated dataset,
    % preallocated for speed
    RHO_b_collection = zeros(B,1); 
        
%     Now comes the permutation step, where the order of the samples in one
%     view (in this case the Y matrix) gets permuted and the algorithm is
%     trained on the permuted data set with a destroyed relationship
%     between X and Y, this process is repeated B times

    for b=1:B

        % use the permuted perm_data_y and the regular keep_in_data_x
        % and compute u_b and v_b for the permutated data using spls
        [u_b, v_b, ~]=spls(keep_in_data_x, dp_perm(keep_in_data_y), cu_opt, cv_opt);

        % compute the absolute correlation between the hold_out data
        % and the permuted u_b and v_b
        RHO_b = abs(corr(hold_out_data_x*u_b,hold_out_data_y*v_b));

        % store the calculated RHO_b value in a vector
        RHO_b_collection(b,1) = RHO_b;
    end
    
    % test the following null hypothesis H_s: "There is no relationship
    % between the two views, therefore the correlation obtained with
    % the original data is not different from the correlation
    % obtained with the permuted data"

    % calculate how many times RHO_b was bigger than or equal to RHO_opt,
    % ie how many times the permutated data led to a higher correlation
    % using spls than the original data
    RHO_count_b = sum(RHO_b_collection >= RHO_opt);

    % calculate the p value to test whether the null hypothesis is true
    p = ((1+RHO_count_b)/(B+1));
  
    % store all parameters of the w-th wide loop into a matrix
    opt_parameters(w,:) = {w cu_opt cv_opt u_opt v_opt success_opt RHO_opt p};
    
    end
    
    %% test the omnibus hypothesis using the p values

    % Omnibus hypothesis H_omni: "All the null hypotheses H_s are true" if
    % any of the p values are lower than the FWE-corrected threshold,
    % then the omnibus hypothesis can be rejected after that. 
    
    % Search for the statistically significant w-th iteration with the
    % lowest p value, if more than one iteration has the same p value,
    % select the one that has the highest RHO value => this is the final
    % w-th iteration with the corresponding cu/cv features and u/v scores
    
    %create a logical indexing the lowest p-value(s)
    p_min = cell2mat(opt_parameters(:,index_p)) == min(cell2mat(opt_parameters(:,index_p))); 
    temp1 = opt_parameters(p_min,:); % placeholder to make syntax easier
    if size(temp1,1) > 1 % if there is more than one p-value that is equal to the lowest p-value, ie there are two absolute minimal p-values, then continue in the loop
        RHO_max = cell2mat(temp1(:,index_RHO)) == max(cell2mat(temp1(:,index_RHO))); % logical indexing the highest RHO value
        final_parameters(ff,:) = temp1(RHO_max,:); % take the row with the lowest p-value and the highest RHO value
    else
        final_parameters(ff,:) = temp1; % if there is just one absolute minimal p value, then take this row
    end

    %% Matrix Deflation => Projection Deflation

    % This method removes the covariance explained by u and v from X
    % and Y by projecting the data matrices onto the space spanned by the
    % corresponding weight vector, and subtracting this from the data:

    u = final_parameters{ff,index_u}; % weight vector for X
    v = final_parameters{ff,index_v}; % weight vector for Y
    X = X - (X*u)*u'; % the effect of u on X is removed by projection deflation
    Y = Y - (Y*v)*v'; % the effect of v on Y is removed by projection deflation
    
    % the p value of this LV is updated from the previously set value of
    % zero to the actual p value of the LV, if this p value is lower than
    % the FWE_rate then the while loop continues after matrix deflation and
    % it keeps looking for the next possible significant LV. If the p value
    % is higher than the FWE_rate, then the while loop is discontinued and
    % the algorithm stops. Therefore, this function generates all possible
    % significant LVs and also the first non-significant LV, afterwards the
    % algorithm stops.
    p_LV = final_parameters{ff,index_p}; 
    
    
    %% Projection of the original data onto the SPLS latent space
    
    % epsilon and omega are the projections of the original data onto the
    % SPLS latent space, spanned by the weight vector pair u and v of this
    % LV. Therefore each LV spans a specific latent space onto wihich the
    % samples can be projected. These latent spaces can then be used to
    % stratify patients.
    epsilon(ff,:) = X_original * final_parameters{ff,index_u};
    omega(ff,:) = Y_original * final_parameters{ff,index_v};

    % ff counts through the most outside LV while loop = counts the amount
    % of significant LVs
    ff = ff+1; 
    

end

% after the LVs are computed, clean up the final parameters, epsilon and
% omega matrix by removing empty rows
final_parameters(ff:size(final_parameters,1),:) = [];
epsilon(ff:size(final_parameters,1),:) = [];
omega(ff:size(final_parameters,1),:) = [];

end

